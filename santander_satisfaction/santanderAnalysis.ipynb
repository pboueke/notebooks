{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import scipy as sp\n",
    "import scipy.stats\n",
    "import sklearn as sk\n",
    "import xgboost as xgb\n",
    "from sklearn import tree\n",
    "from sklearn import metrics\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "from sklearn.ensemble import RandomForestClassifier"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Primeiramente, vamos ler o dataset de treino e armazenar suas informações em memória. Guardamos também uma lista de identificadores de colunas, que relacionam o index de cada coluna no dataset com o index original de cada coluna. Agora no início essa relação é de completa igualdade.\n",
    "\n",
    "Guardamos também o índice da coluna referente ao target de cada registro."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "dataset = []\n",
    "target_column = -1\n",
    "with open(\"datasets/train_file.csv\", 'r') as f:\n",
    "    header = f.readline()\n",
    "    header = header.split(',')\n",
    "    for line in f:\n",
    "        data = line[:-1].split(',')\n",
    "        # Usamos float para tratar possíveis casos de números fracionários em meio a registros inteiros\n",
    "        data = list(map(float, data))\n",
    "        dataset.append(data)\n",
    "dataset = np.array(dataset)\n",
    "columnsId = list(range((dataset.shape)[1]))\n",
    "target_column = len(columnsId)-1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Observamos em uma análise externa a este caderno que algumas colunas possuem todos os seus registros iguais. Portanto, queremos que essas colunas não façam parte do dataset final de treino, pois não acrescentam em nada aos resultados e deixam o processamento mais lento por aumentar o tamanho da matriz.\n",
    "\n",
    "Para isso, marcamos todas as colunas que possuem desvio padrão dos seus registros igual a zero, e deletamos elas do dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "columnsToDelete = []\n",
    "eps = 1e-10\n",
    "for i in range((dataset.shape)[1]):\n",
    "    column = dataset[:, i]\n",
    "    stderr = np.std(column)\n",
    "    if stderr < eps:\n",
    "        columnsToDelete.append(i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "dataset = np.delete(dataset, columnsToDelete, 1)\n",
    "# Deletamos algumas colunas, precisamos deletar os índiced dessas colunas em nossa relação de índices\n",
    "for col in columnsToDelete[::-1]:\n",
    "    del(columnsId[col])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Para explorar ao máximo a informação que foi dada no dataset, fizemos uma outra análise externa a este notebook que apontou para a necessidade de aplicarmos um hashing trick no nosso dataset, a fim de eliminar colunas cujos valores numéricos não representem relação de ordem de grandeza (1 não necessariamente é menor do que 2, se considerarmos o que aquela coluna significa), e substitui-las por colunas que formem um espaço vetorial das possíveis entradas da coluna original. Por exemplo:\n",
    "\n",
    "Uma coluna possui somente valores 0, 1, 2, 5, 6, 8 e 9. Podemos dividir essa coluna em outras 7 colunas binárias, onde a presença de um 1 em uma coluna implica que o valor original daquele registro era igual ao valor associado à sua nova coluna. Assim:\n",
    "\n",
    "$$X - 0,1,2,5,6,8,9$$\n",
    "$$---------$$\n",
    "$$0 - 1,0,0,0,0,0,0$$\n",
    "$$0 - 1,0,0,0,0,0,0$$\n",
    "$$1 - 0,1,0,0,0,0,0$$\n",
    "$$1 - 0,1,0,0,0,0,0$$\n",
    "$$2 - 0,0,1,0,0,0,0$$\n",
    "$$6 - 0,0,0,0,1,0,0$$\n",
    "$$5 - 0,0,0,1,0,0,0$$\n",
    "$$8 - 0,0,0,0,0,1,0$$\n",
    "$$9 - 0,0,0,0,0,0,1$$\n",
    "$$0 - 1,0,0,0,0,0,0$$\n",
    "$$1 - 0,1,0,0,0,0,0$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Para tanto, contamos e armazenamos quantos valores únicos uma coluna possui em um dicionário. Se tivermos 7 ou menos registros únicos, aplicamos nossa técnica de hashing trick. Armazenamos quais colunas iremos \"expandir\" em outras."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "columnsToExpand = []\n",
    "for i in range((dataset.shape)[1]-1):\n",
    "    counter = {}\n",
    "    column = dataset[:, i]\n",
    "    for element in column:\n",
    "        if element in counter:\n",
    "            counter[element] += 1\n",
    "        else:\n",
    "            counter[element] = 1\n",
    "    if len(counter.keys()) <= 7:\n",
    "        keys = list(counter.keys())\n",
    "        if keys == list(map(int, keys)):\n",
    "            columnsToExpand.append((i, counter))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "newcolumns = []\n",
    "for col, counter in columnsToExpand:\n",
    "    column = dataset[:, col]\n",
    "    for key in counter.keys():\n",
    "        # Adicionamos nossas novas colunas na nossa relação de índices\n",
    "        columnsId.append((col, key))\n",
    "        newcolumns.append([1 if el == key else 0 for el in column])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Não podemos nos esquecer de apagar as colunas originais!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "dataset = np.delete(dataset, [col[0] for col in columnsToExpand], 1)\n",
    "for col, counter in columnsToExpand[::-1]:\n",
    "    # Removemos as colunas originais da relação de índices\n",
    "    del(columnsId[col])\n",
    "# Precisamos atualizar quem é a coluna de target, agora que mexemos na nossa relação de índices\n",
    "target_column = columnsId.index(target_column)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "E agora, unir o dataset antigo com as novas colunas:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "dimensions = dataset.shape\n",
    "newdataset = np.empty((dimensions[0], dimensions[1]+len(newcolumns)))\n",
    "newcolumns = np.transpose(np.array(newcolumns))\n",
    "newdataset[:, :dimensions[1]] = dataset\n",
    "newdataset[:, dimensions[1]:] = newcolumns"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Vamos agora criar nosso modelo usando a biblioteca XGBoost (Extreme Gradient Boosting), usando nosso novo dataset com o hashing trick. Os parâmetros usados também são especificados abaixo:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 140,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "param = {\"objective\":\"binary:logistic\", \"booster\":\"gbtree\", \"eval_metric\":\"auc\", \"nthread\":4, \"scale_pos_weight\":1,\n",
    "         \"eta\":0.0202048, \"max_depth\":6, \"subsample\":0.6815, \"colsample_bytree\":0.701, \"seed\":111}\n",
    "# Lembrando que precisamos remover a coluna de target do dataset, e usa-la somente para especificar os targets no train\n",
    "dtrain = xgb.DMatrix(np.delete(newdataset, target_column, 1), label=list(newdataset[:, target_column].astype(\"uint8\")))\n",
    "bst = xgb.train(param, dtrain, 312)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Uma vez treinado, o modelo poderá ser aplicado no dataset de teste. Portanto, realizaremos o mesmo procedimento anterior para carregar o dataset de teste em memória:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 141,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "test_dataset = []\n",
    "with open(\"datasets/test_file.csv\", 'r') as f:\n",
    "    f.readline()\n",
    "    for line in f:\n",
    "        data = line[:-1].split(',')\n",
    "        data = list(map(float, data))\n",
    "        test_dataset.append(data)\n",
    "test_dataset = np.array(test_dataset)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Devemos deletar todas as colunas que deletamos do dataset original, e também realizar o mesmo procedimento de hashing trick descrito anteriormente no nosso dataset de teste."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 142,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "for col in columnsToDelete[::-1]:\n",
    "    test_dataset = np.delete(test_dataset, col, 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 143,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "newcolumns = []\n",
    "for col, counter in columnsToExpand:\n",
    "    column = test_dataset[:, col]\n",
    "    for key in counter.keys():\n",
    "        newcolumns.append([1 if el == key else 0 for el in column])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 144,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "for col, counter in columnsToExpand[::-1]:\n",
    "    test_dataset = np.delete(test_dataset, col, 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 145,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "dimensions = test_dataset.shape\n",
    "newtestdataset = np.empty((dimensions[0], dimensions[1]+len(newcolumns)))\n",
    "newcolumns = np.transpose(np.array(newcolumns))\n",
    "newtestdataset[:, :dimensions[1]] = test_dataset\n",
    "newtestdataset[:, dimensions[1]:] = newcolumns"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finalmente, podemos usar nosso modelo para prever a probabilidade de cada registro ser da classe 1:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 146,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "dtest = xgb.DMatrix(newtestdataset)\n",
    "y_pred = bst.predict(dtest)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "E por fim, armazenar o resultado em um arquivo de texto para ser carregado no site da competição:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 147,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "with open(\"result.txt\", 'w') as f:\n",
    "    for pred in y_pred:\n",
    "        f.write(str(pred)+'\\n')\n",
    "f.close()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.4.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
